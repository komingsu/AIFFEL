{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a12916-5c80-4fec-a738-fc0aabb4e5b6",
   "metadata": {},
   "source": [
    "# 데이터 다듬기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae14c09-9848-4e6c-8c57-196bdeeaaf46",
   "metadata": {},
   "source": [
    "데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676ac18d-4ab4-49f9-892e-8f794de9ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# os: Operating System의 줄임말로, 운영체제에서 제공되는 여러 기능을 파이썬에서 사용할 수 있도록 함 (Ex. 디렉토리 경로 이동, 시스템 환경 변수 가져오기 등)\n",
    "# re: Regular Expression의 줄임말로, 파이썬 정규표현식을 사용하기 위한 모듈\n",
    "# numpy(NumPy): 행렬이나 대규모 다차원 배열을 쉽게 처리할 수 있도록 지원하는 라이브러리. 데이터 구조 외에도 수치 계산을 위해 효율적으로 구현된 기능을 제공\n",
    "# tensorflow(TensorFlow): 데이터 흐름 프로그래밍을 위한 라이브러리. 데이터 그래프를 수치적으로 연산하기 위해 구글에서 만든 오픈 소스 라이브러리.\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "file_path = '../data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# sample\n",
    "for i in raw_corpus[:9]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851db3d1-72a5-4b39-bb9e-e49530e42abc",
   "metadata": {},
   "source": [
    "문장 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64bfeb78-d2c8-404d-838b-a646072ed600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() # 5. 다시 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    return sentence\n",
    "\n",
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    # 필요없는 문장제거\n",
    "    if len(sentence) == 0: continue # 없는 문장\n",
    "    if sentence[-1] == \":\": continue # 끝이 : 인 문장\n",
    "    \n",
    "    # 문장 cleansing\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    # 담기\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cafe3-2c99-455f-b453-c8f539f163f9",
   "metadata": {},
   "source": [
    " 단어 사전 생성 및 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9231ce-aa13-436a-a582-a15a63609c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 텐서 sample 3개\n",
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n",
      "\n",
      "단어 사전 단어 sample 10개\n",
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n",
      "\n",
      "input, target 샘플\n",
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, # 토큰화할 단어 수\n",
    "        filters=' ', # 문장을 정제할 필터함수\n",
    "        oov_token=\"<unk>\" # 모르는 단어 처리\n",
    "    )\n",
    "    # 사전 제작\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 토큰화된 문장\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    # 문장의 길이 맞추기\n",
    "    # padding='pre'은 문장 앞에 padding, 'post'는 문장 뒤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=20)  \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "# sample\n",
    "print(\"토큰화된 텐서 sample 3개\")\n",
    "print(tensor[:3, :10])\n",
    "\n",
    "# 단어 사전 sample\n",
    "print(\"\\n단어 사전 단어 sample 10개\")\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx >= 10: break\n",
    "\n",
    "    \n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "# input, target - sample\n",
    "print(\"\\ninput, target 샘플\")\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09730b2c-b8c6-4ab0-b7e6-309ed899b306",
   "metadata": {},
   "source": [
    "# 언어 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0190db-a95f-488d-8c53-cb1f39e7200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(256, 19), dtype=tf.int32, name=None), TensorSpec(shape=(256, 19), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# 단어 사전 크기 ( 7000 + 1 <0:pad> )\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# [input x, target y] 형태의 데이터셋 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252d16c1-bf2e-4813-92b3-54582b270905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  1792256   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               multiple                  5246976   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               multiple                  8392704   \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  7176025   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
    "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size , hidden_size)\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "# class api로 모델을 부를때는 데이터를 넣어줘야 summary를 확인할 수 있습니다.\n",
    "model(src_sample)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1876705-5797-4b3e-b0e2-12d1f7b4b0a5",
   "metadata": {},
   "source": [
    " 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a2b753-df95-4718-9fff-f2cc322a78ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 10s 88ms/step - loss: 3.6179\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 8s 89ms/step - loss: 2.9447\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 8s 89ms/step - loss: 2.8360\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 8s 89ms/step - loss: 2.7353\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 8s 89ms/step - loss: 2.6715\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.6181\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.5576\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.5018\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.4480\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.3969\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.3480\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.2994\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 2.2492\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 2.2000\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 2.1509\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.1029\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.0525\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 2.0027\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.9542\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.9052\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 8s 90ms/step - loss: 1.8557\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.8053\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.7558\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.7053\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.6552\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 9s 92ms/step - loss: 1.6044\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.5528\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.5008\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.4487\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 8s 91ms/step - loss: 1.3945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3240f7e20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, # 기본값은 False이다. True이면 softmax함수가 적용되지 않았다는걸 의미한다. \n",
    "    reduction='none'  # 기본값은 SUM이다. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",
    ")\n",
    "# 모델을 학습시키키 위한 학습과정을 설정하는 단계이다.\n",
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정했다.\n",
    "model.fit(dataset, epochs=30) # 만들어둔 데이터셋으로 모델을 학습한다. 30번 학습을 반복하겠다는 의미다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a856f5c-2b0c-41a4-86ac-ff9875bd24ac",
   "metadata": {},
   "source": [
    "# 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "331d4c93-fe1a-4b10-9209-312fead93a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    while True:\n",
    "        predict = model(test_tensor) # 1. 입력받은 문장의 텐서를 입력합니다\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # 3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "        \n",
    "        # 4.모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867a7fc1-1c31-4ceb-aca8-72de998d73eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a disease indeed , that baes like a man . <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\") # 시작문장으로 he를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bac3f6-8726-4523-b344-12efd6b67bea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd988ee-5246-45a2-9918-7f4823ad635b",
   "metadata": {},
   "source": [
    "# EX TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67e53e-19f9-4c0d-a35a-8eb7a1f014cb",
   "metadata": {},
   "source": [
    "* 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03df2d09-0603-4572-bb30-3829d27d0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466bd778-399c-42c8-8cf7-0e4f1417efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = '../data/lyrics/*' #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
    "\n",
    "txt_list = glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc9047-fb67-49dd-8e8a-140cf2d097bd",
   "metadata": {},
   "source": [
    "* 문장 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811dc096-e8bc-4257-beab-2c03641e3cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> looking for some education <end>',\n",
       " '<start> made my way into the night <end>',\n",
       " '<start> all that bullshit conversation <end>',\n",
       " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
       " '<start> i don t even wanna waste your time <end>',\n",
       " '<start> let s just say that maybe <end>',\n",
       " '<start> you could help me ease my mind <end>',\n",
       " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
       " '<start> if that s love in your eyes <end>',\n",
       " '<start> it s more than enough <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue \n",
    "    if sentence[-1] == \":\": continue \n",
    "\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1eae78-edd7-40b2-91cb-4161a17f2663",
   "metadata": {},
   "source": [
    "* 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76c8fb2-2451-4708-abb1-729d9fa623fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # 단어 수\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=20) # 최대 길이 15, 앞쪽 padding\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b05490-5440-4863-9d9e-5e17e2957031",
   "metadata": {},
   "source": [
    "* TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1beb11-341c-4f46-9207-7baac1c40336",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "919c0602-4922-4e57-ae74-9ad8e96e3a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2,    5,   22,    6,  228,  182,   14, 1575,  673, 5027,    3,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " array([   5,   22,    6,  228,  182,   14, 1575,  673, 5027,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " array([  2, 678,   4, 678,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0], dtype=int32),\n",
       " array([678,   4, 678,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0], dtype=int32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0], dec_train[0], enc_val[0], dec_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284ec2c-983a-4cca-b15f-d34879b6b8d8",
   "metadata": {},
   "source": [
    "* 언어 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344c49c2-9a40-47b3-9732-36ceb6a3a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  48004000  \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               multiple                  4359168   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               multiple                  525312    \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  3084257   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,972,737\n",
      "Trainable params: 55,972,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "embedding_size = 4000\n",
    "hidden_size = 256\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size , hidden_size)\n",
    "\n",
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "\n",
    "model(src_sample)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fa302b-ccf2-40ab-8ba6-ffe9b9f87e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 39s 69ms/step - loss: 3.1775 - val_loss: 2.8639\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.7750 - val_loss: 2.6955\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.5944 - val_loss: 2.5295\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.4483 - val_loss: 2.4173\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.3367 - val_loss: 2.3306\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.2458 - val_loss: 2.2685\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.1697 - val_loss: 2.2185\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.1000 - val_loss: 2.1750\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 2.0348 - val_loss: 2.1388\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 38s 69ms/step - loss: 1.9746 - val_loss: 2.1099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a54ba8e80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, validation_data=test_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e32e3-aa56-45dd-ba38-6d8666f43ced",
   "metadata": {},
   "source": [
    "* 문장생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a5e9c0-ac50-4a9f-ad91-b10ef107690d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a <unk> , i m a <unk> <end> '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    while True:\n",
    "        predict = model(test_tensor) # 1. 입력받은 문장의 텐서를 입력합니다\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # 3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "        \n",
    "        # 4.모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> he\") # 시작문장으로 he를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79817d2b-a4a6-4fb5-9819-fca8cbcdd74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081a47c-c7b5-4415-8805-b07d1ab9d491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
